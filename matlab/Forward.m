function [out, act_h, act_a] = Forward(W, b, X)
% [OUT, act_h, act_a] = Forward(W, b, X) performs forward propogation on the
% input data 'X' uisng the network defined by weights and biases 'W' and 'b'
% (as generated by InitializeNetwork(..)).
%
% This function should return the final softmax output layer activations in OUT,
% as well as the hidden layer post activations in 'act_h', and the hidden layer
% pre activations in 'act_a'.
m=size(W,2);
act_a={};
act_h={};
%X=transpose(X);
for i = 1:1:m     
    X=X* W{1,i}+b{1,i};    
    act_a{1,i}=X;
    X=sigmf(X, [1 0]);
    act_h{1,i}=X;    
end
exp_x=exp(X);
sum_exp=sum(exp_x);
out=exp_x/sum_exp;
end
